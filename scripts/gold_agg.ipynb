{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2d01cb-6f6e-40ff-9a29-f2e485ccb10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, explode, from_json, col, current_date,current_timestamp, lit, to_date, month\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, StructType, StructField, ArrayType\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99c0881-c071-4e80-9b0c-96b2c44d52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_URI = \"http://nessie:19120/api/v1\"\n",
    "WAREHOUSE = \"s3a://gold/\"              \n",
    "STORAGE_URI = \"http://minio:9000\"\n",
    "AWS_ACCESS_KEY = \"admin\"\n",
    "AWS_SECRET_KEY = \"password\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed0dd85-ce6d-4d5e-895e-e1a01ea68874",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('silver_transform')\n",
    "\n",
    "        # 📦 Dependencias necesarias\n",
    "        .set(\"spark.jars.packages\", \",\".join([\n",
    "            \"org.postgresql:postgresql:42.7.3\",\n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0\",\n",
    "            \"org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1\",\n",
    "            \"software.amazon.awssdk:bundle:2.24.8\",\n",
    "            \"software.amazon.awssdk:url-connection-client:2.24.8\",\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4\"\n",
    "        ]))\n",
    "\n",
    "        # 🧩 Extensiones Iceberg + Nessie\n",
    "        .set(\"spark.sql.extensions\", \",\".join([\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "            \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n",
    "        ]))\n",
    "\n",
    "        # 🗂️ Catálogo Nessie\n",
    "        .set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "        .set(\"spark.sql.catalog.nessie.uri\", CATALOG_URI)\n",
    "        .set(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "        .set(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "        .set(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\")\n",
    "        .set(\"spark.sql.catalog.nessie.warehouse\", WAREHOUSE)\n",
    "\n",
    "        # ☁️ Configuración S3A para MinIO\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", STORAGE_URI)\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "        .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "             \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "        .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "        # ⚡ Optimizaciones de ejecución\n",
    "        .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .set(\"spark.sql.parquet.filterPushdown\", \"true\")\n",
    "        .set(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"64\")  # 🔧 más particiones para distribuir carga\n",
    "        .set(\"spark.sql.files.maxPartitionBytes\", \"64MB\")  # ⚖️ reduce tamaño de tarea para evitar saturación\n",
    "\n",
    "        .set(\"spark.driver.memory\", \"5g\")                     # Driver usa hasta 5 GB (de los 6g disponibles)\n",
    "        .set(\"spark.executor.memory\", \"6g\")                   # Cada executor usa hasta 6 GB (de los 8g disponibles)\n",
    "        .set(\"spark.executor.cores\", \"4\")                     # Más núcleos por executor para paralelismo\n",
    "        .set(\"spark.driver.maxResultSize\", \"2g\")              # Aumenta el límite de resultados del driver\n",
    "        .set(\"spark.network.timeout\", \"600s\")                 # Timeout más generoso para cargas grandes\n",
    "        .set(\"spark.executor.heartbeatInterval\", \"60s\")       # Latido coherente con el timeout\n",
    "        \n",
    "        # ⚙️ Escritura\n",
    "        .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235a290e-595d-4f91-b69d-75675d4e38e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Started\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Session Started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e7f4cb9-922a-4175-8623-fafb4a441fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = spark.read.table(\"nessie.silver.posts\")\n",
    "votes_df = spark.read.table(\"nessie.silver.votes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6c97af-4cd1-413a-9715-0984bd5ccadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df.createOrReplaceTempView(\"posts_view\")\n",
    "votes_df.createOrReplaceTempView(\"votes_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7cf4ee-7e83-445b-b370-2fd4d98e5f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_stats_per_post = spark.sql(\"\"\"\n",
    "    WITH vote_summary AS (\n",
    "        SELECT\n",
    "            v.post_id_clean AS post_id,\n",
    "            -- Clasificamos votos positivos / negativos\n",
    "            SUM(CASE WHEN v.vote_type_id_clean IN (2, 18, 20, 21, 32) THEN 1 ELSE 0 END) AS upvotes,\n",
    "            SUM(CASE WHEN v.vote_type_id_clean IN (3, 33) THEN 1 ELSE 0 END) AS downvotes,\n",
    "            COUNT(*) AS total_votes,\n",
    "            YEAR(v.creation_ts) AS year,\n",
    "            MONTH(v.creation_ts) AS month\n",
    "        FROM votes_view v\n",
    "        GROUP BY v.post_id_clean, YEAR(v.creation_ts), MONTH(v.creation_ts)\n",
    "    ),\n",
    "    posts_enriched AS (\n",
    "        SELECT\n",
    "            p.Id AS post_id,\n",
    "            p.PostTypeId,\n",
    "            p.CreationDate,\n",
    "            p.LastActivityDate,\n",
    "            CASE p.PostTypeId\n",
    "                WHEN 1 THEN 'Question'\n",
    "                WHEN 2 THEN 'Answer'\n",
    "                WHEN 3 THEN 'Orphaned Tag Wiki'\n",
    "                WHEN 4 THEN 'Tag Wiki Excerpt'\n",
    "                WHEN 5 THEN 'Tag Wiki'\n",
    "                WHEN 6 THEN 'Moderator Nomination'\n",
    "                WHEN 7 THEN 'Wiki Placeholder'\n",
    "                WHEN 8 THEN 'Privilege Wiki'\n",
    "                WHEN 9 THEN 'Article'\n",
    "                WHEN 10 THEN 'Help Article'\n",
    "                WHEN 12 THEN 'Collection'\n",
    "                WHEN 13 THEN 'Moderator Questionnaire Response'\n",
    "                WHEN 14 THEN 'Announcement'\n",
    "                WHEN 15 THEN 'Collective Discussion'\n",
    "                WHEN 17 THEN 'Collective Collection'\n",
    "                ELSE 'Other'\n",
    "            END AS post_type\n",
    "        FROM posts_view p\n",
    "    )\n",
    "    SELECT\n",
    "        ps.post_id,\n",
    "        ps.post_type,\n",
    "        vs.year,\n",
    "        vs.month,\n",
    "        vs.upvotes,\n",
    "        vs.downvotes,\n",
    "        vs.total_votes,\n",
    "        (vs.upvotes - vs.downvotes) AS score,\n",
    "        ROUND(CASE WHEN vs.total_votes > 0 THEN vs.upvotes / vs.total_votes ELSE 0 END, 3) AS upvote_pct,\n",
    "        ROUND(CASE WHEN vs.total_votes > 0 THEN vs.downvotes / vs.total_votes ELSE 0 END, 3) AS downvote_pct\n",
    "    FROM vote_summary vs\n",
    "    JOIN posts_enriched ps\n",
    "      ON vs.post_id = ps.post_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "372eaf54-c289-48ec-af30-ae23b43afd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac55af4-a983-4660-8f88-308d67e9fdc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
