{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccbc1fed-a1ed-4414-a0d2-2b454ed903c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, explode, from_json, col, current_date,current_timestamp, lit\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, StructType, StructField, ArrayType\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b7c792-b881-457a-ab40-640e1003ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_URI = \"http://nessie:19120/api/v1\"\n",
    "WAREHOUSE = \"s3a://silver/\"              \n",
    "STORAGE_URI = \"http://minio:9000\"\n",
    "AWS_ACCESS_KEY = \"admin\"\n",
    "AWS_SECRET_KEY = \"password\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9e79275-57d1-43d0-a8fc-6ca01db10936",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('silver_transform')\n",
    "\n",
    "        # ðŸ“¦ Dependencias necesarias\n",
    "        .set(\"spark.jars.packages\", \",\".join([\n",
    "            \"org.postgresql:postgresql:42.7.3\",\n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0\",\n",
    "            \"org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1\",\n",
    "            \"software.amazon.awssdk:bundle:2.24.8\",\n",
    "            \"software.amazon.awssdk:url-connection-client:2.24.8\",\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4\"\n",
    "        ]))\n",
    "\n",
    "        # ðŸ§© Extensiones Iceberg + Nessie\n",
    "        .set(\"spark.sql.extensions\", \",\".join([\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "            \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n",
    "        ]))\n",
    "\n",
    "        # ðŸ—‚ï¸ CatÃ¡logo Nessie\n",
    "        .set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "        .set(\"spark.sql.catalog.nessie.uri\", CATALOG_URI)\n",
    "        .set(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "        .set(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "        .set(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\")\n",
    "        .set(\"spark.sql.catalog.nessie.warehouse\", WAREHOUSE)\n",
    "\n",
    "        # â˜ï¸ ConfiguraciÃ³n S3A para MinIO\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", STORAGE_URI)\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "        .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "             \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "        .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "        # âš¡ Optimizaciones de ejecuciÃ³n\n",
    "        .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .set(\"spark.sql.parquet.filterPushdown\", \"true\")\n",
    "        .set(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"64\")  # ðŸ”§ mÃ¡s particiones para distribuir carga\n",
    "        .set(\"spark.sql.files.maxPartitionBytes\", \"64MB\")  # âš–ï¸ reduce tamaÃ±o de tarea para evitar saturaciÃ³n\n",
    "\n",
    "        # ðŸ”§ Recursos\n",
    "        .set(\"spark.driver.memory\", \"12g\")             # mÃ¡s memoria para driver\n",
    "        .set(\"spark.executor.memory\", \"12g\")           # mÃ¡s memoria por executor\n",
    "        .set(\"spark.executor.cores\", \"4\")              # mÃ¡s cores por executor\n",
    "        .set(\"spark.driver.maxResultSize\", \"6g\")       # mayor tolerancia al traer datos grandes\n",
    "        .set(\"spark.network.timeout\", \"600s\")          # â±ï¸ evita desconexiÃ³n por tareas largas\n",
    "        .set(\"spark.executor.heartbeatInterval\", \"60s\")# â¤ï¸ mantiene conexiÃ³n activa\n",
    "\n",
    "        # âš™ï¸ Escritura\n",
    "        .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f8ab72d-5b3c-4ba3-875d-b7523461f276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Started\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Session Started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a79a110-b3f5-4714-b5db-57eae55aed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Revisando s3a://bronze/posts/\n",
      "ðŸ“¦ 2 Parquet files found inposts:\n",
      "   ðŸ“„ s3a://bronze/posts/2022/2022.parquet\n",
      "   ðŸ“„ s3a://bronze/posts/2023/2023.parquet\n",
      "\n",
      "ðŸ” Revisando s3a://bronze/votes/\n",
      "ðŸ“¦ 2 Parquet files found invotes:\n",
      "   ðŸ“„ s3a://bronze/votes/_2022/1760206920.323453.f8f40d786f.parquet\n",
      "   ðŸ“„ s3a://bronze/votes/_2023/1760206962.9017577.6d111cba5f.parquet\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from py4j.java_gateway import java_import\n",
    "\n",
    "    # Import Hadoop classes\n",
    "    java_import(spark._jvm, 'org.apache.hadoop.fs.FileSystem')\n",
    "    java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "    java_import(spark._jvm, 'java.net.URI')\n",
    "\n",
    "    # âœ… Create S3A-aware FileSystem\n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark._jvm.java.net.URI(\"s3a://bronze/\"),\n",
    "        spark._jsc.hadoopConfiguration()\n",
    "    )\n",
    "\n",
    "    def list_parquet_files(base_path):\n",
    "        results = []\n",
    "        path = spark._jvm.org.apache.hadoop.fs.Path(base_path)\n",
    "\n",
    "        if not fs.exists(path):\n",
    "            print(f\"âš ï¸ Path does not exist: {base_path}\")\n",
    "            return results\n",
    "\n",
    "        def recurse(p):\n",
    "            try:\n",
    "                status_list = fs.listStatus(p)\n",
    "                for f in status_list:\n",
    "                    if f.isDirectory():\n",
    "                        recurse(f.getPath())\n",
    "                    elif f.getPath().getName().endswith(\".parquet\"):\n",
    "                        results.append(f.getPath().toString())\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error listing {p}: {e}\")\n",
    "\n",
    "        recurse(path)\n",
    "        return results\n",
    "\n",
    "    # ===============================\n",
    "    # List posts and votes folders\n",
    "    # ===============================\n",
    "    for subdir in [\"posts\", \"votes\"]:\n",
    "        base = f\"s3a://bronze/{subdir}/\"\n",
    "        print(f\"\\nðŸ” Revisando {base}\")\n",
    "\n",
    "        try:\n",
    "            path = spark._jvm.org.apache.hadoop.fs.Path(base)\n",
    "            status = fs.listStatus(path)\n",
    "\n",
    "            if len(status) == 0:\n",
    "                print(\"âš ï¸ The bucket exists but is empty.\")\n",
    "            else:\n",
    "                parquet_files = list_parquet_files(base)\n",
    "                if parquet_files:\n",
    "                    print(f\"ðŸ“¦ {len(parquet_files)} Parquet files found in{subdir}:\")\n",
    "                    for f in parquet_files:\n",
    "                        print(\"   ðŸ“„\", f)\n",
    "                else:\n",
    "                    print(\"âš ï¸ Not .parquet files found\", base)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"âŒ Error listing bucket:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e1fd5f9-742c-47f7-8975-ee21dfe10049",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_paths = list_parquet_files(\"s3a://bronze/posts/\")\n",
    "votes_paths = list_parquet_files(\"s3a://bronze/votes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2b06d86-9b18-4d66-b1d9-de29686fe64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_year(paths, year):\n",
    "    pattern = re.compile(rf\"[_/\\.]{year}([_/\\.]|$)\")\n",
    "    return [p for p in paths if pattern.search(p)]\n",
    "\n",
    "votes_2022_paths = filter_by_year(votes_paths, 2022)\n",
    "votes_2023_paths = filter_by_year(votes_paths, 2023)\n",
    "posts_2022_paths = filter_by_year(posts_paths, 2022)\n",
    "posts_2023_paths = filter_by_year(posts_paths, 2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63235c72-4055-41e8-98e8-613fcd289401",
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_2022 = spark.read.parquet(*votes_2022_paths)\n",
    "votes_2023 = spark.read.parquet(*votes_2023_paths)\n",
    "posts_2022 = spark.read.parquet(*posts_2022_paths)\n",
    "posts_2023 = spark.read.parquet(*posts_2023_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f8de65-f287-4fda-9f18-6de6c3c2e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_posts(df):\n",
    "    \"\"\"\n",
    "    Limpieza y enriquecimiento temporal de POSTS (Silver Layer).\n",
    "    - Convierte CreationDate a timestamp y genera columnas auxiliares.\n",
    "    - AÃ±ade columnas: creation_date_str, year, load_date.\n",
    "    - Rellena valores nulos, normaliza tipos y elimina duplicados.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"ðŸ§¹ Cleaning & enriching POSTS dataset...\")\n",
    "\n",
    "        \n",
    "        # Eliminar duplicados\n",
    "        df = df.dropDuplicates([\"Id\"])\n",
    "\n",
    "        # Asegurar tipos numÃ©ricos coherentes\n",
    "        df = df.withColumn(\"Score\", F.coalesce(F.col(\"Score\").cast(\"long\"), F.lit(0))) \\\n",
    "               .withColumn(\"ViewCount\", F.coalesce(F.col(\"ViewCount\").cast(\"long\"), F.lit(0))) \\\n",
    "               .withColumn(\"AnswerCount\", F.coalesce(F.col(\"AnswerCount\").cast(\"long\"), F.lit(0))) \\\n",
    "               .withColumn(\"CommentCount\", F.coalesce(F.col(\"CommentCount\").cast(\"long\"), F.lit(0))) \\\n",
    "               .withColumn(\"FavoriteCount\", F.coalesce(F.col(\"FavoriteCount\").cast(\"long\"), F.lit(0)))\n",
    "\n",
    "        # Convertir 'Body' a string y reemplazar nulos por vacÃ­o\n",
    "        df = df.withColumn(\"Body\", F.col(\"Body\").cast(\"string\"))\n",
    "        df = df.withColumn(\"Body\", F.when(F.col(\"Body\").isNull(), F.lit(\"\")).otherwise(F.col(\"Body\")))\n",
    "\n",
    "        # Convertir fechas\n",
    "        date_columns = [\n",
    "            \"CreationDate\", \"LastEditDate\", \"LastActivityDate\",\n",
    "            \"CommunityOwnedDate\", \"ClosedDate\"\n",
    "        ]\n",
    "        for col in date_columns:\n",
    "            if col in df.columns:\n",
    "                df = df.withColumn(col, F.to_timestamp(F.col(col)))\n",
    "\n",
    "        # Agregar columnas de fecha derivadas\n",
    "        df = df.withColumn(\"creation_date\", F.col(\"CreationDate\")) \\\n",
    "               .withColumn(\"creation_date_str\", F.date_format(F.col(\"creation_date\"), \"yyyy-MM-dd\")) \\\n",
    "               .withColumn(\"year\", F.year(F.col(\"creation_date\")))\n",
    "\n",
    "        print(\"âœ… POSTS cleaning & enrichment complete.\\n\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error cleaning POSTS: {e}\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50c10d6e-4a40-4b63-a5ed-95391ad33872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_votes(df):\n",
    "    \"\"\"\n",
    "    Cleans and enriches the VOTES dataset for the Silver layer.\n",
    "    Includes timestamp normalization, temporal enrichment, and null handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"ðŸ§¹ Cleaning & enriching VOTES dataset...\")\n",
    "\n",
    "        # Remove duplicates based on 'id'\n",
    "        df = df.dropDuplicates([\"id\"])\n",
    "\n",
    "        # Handle missing values\n",
    "        # Replace null bounty_amount with median or 0\n",
    "        if df.filter(F.col(\"bounty_amount\").isNotNull()).count() > 0:\n",
    "            median_bounty = df.approxQuantile(\"bounty_amount\", [0.5], 0.1)[0]\n",
    "        else:\n",
    "            median_bounty = 0\n",
    "        df = df.fillna({\"bounty_amount\": median_bounty})\n",
    "\n",
    "        # Replace missing user_id or vote_type_id with -1 (unknown)\n",
    "        df = df.fillna({\"user_id\": -1, \"vote_type_id\": -1, \"post_id\": -1})\n",
    "\n",
    "        # Normalize timestamp and enrich with temporal columns\n",
    "        df = (\n",
    "            df.withColumn(\"creation_date\", F.to_timestamp(F.col(\"creation_date\")))\n",
    "              .withColumn(\"creation_date_str\", F.date_format(F.col(\"creation_date\"), \"yyyy-MM-dd\"))\n",
    "              .withColumn(\"year\", F.year(F.col(\"creation_date\")))\n",
    "              .withColumn(\"load_date\", F.current_timestamp())\n",
    "        )\n",
    "\n",
    "        # 4Filter invalid records\n",
    "        df = df.filter(F.col(\"id\").isNotNull() & F.col(\"creation_date\").isNotNull())\n",
    "\n",
    "        print(\"âœ… VOTES cleaning & enrichment complete.\\n\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error cleaning VOTES: {e}\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dd5b5ee-dc7a-4d40-aa14-1099824b3c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning & enriching VOTES dataset...\n",
      "âœ… VOTES cleaning & enrichment complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "votes_2022_clean = clean_votes(votes_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35e89b94-4d9b-49e7-982c-d22492179cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning & enriching VOTES dataset...\n",
      "âœ… VOTES cleaning & enrichment complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "votes_2023_clean = clean_votes(votes_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "484718e2-a5b9-4714-873c-69483f5a91da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning & enriching POSTS dataset...\n",
      "âœ… POSTS cleaning & enrichment complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_2022_clean = clean_posts(posts_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f26fc589-fc5a-4481-bfd0-dda8aeaf589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Cleaning & enriching POSTS dataset...\n",
      "âœ… POSTS cleaning & enrichment complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_2023_clean = clean_posts(posts_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fda08ace-cf77-444e-9e7a-76f9dbf03021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.silver\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cba6a6bc-8c88-4d24-a7a6-6cb32e2c28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_in_batches(df, table_name, catalog=\"nessie.silver\", repartitions=32, batch_size=50000, max_retries=3):\n",
    "    \"\"\"\n",
    "    Escribe un DataFrame en Nessie en batches usando append para evitar saturar Py4J.\n",
    "    \"\"\"\n",
    "    key_col = next((c for c in [\"id\", \"Id\"] if c in df.columns), None)\n",
    "    df = df.repartition(repartitions, key_col) if key_col else df.repartition(repartitions)\n",
    "\n",
    "    # Evitar .count() directo\n",
    "    df = df.withColumn(\"_row_index\", F.monotonically_increasing_id())\n",
    "    df.cache()\n",
    "    print(\"âœ… DataFrame preparado con Ã­ndice para particionado\")\n",
    "\n",
    "    # Calcular nÃºmero de batches sin usar .count()\n",
    "    indexed_df = df.select(\"_row_index\")\n",
    "    total_rows = indexed_df.count()\n",
    "    print(f\"ðŸ“Š Total rows to write in append: {total_rows}\")\n",
    "\n",
    "    for start in range(0, total_rows, batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_df = df.filter((F.col(\"_row_index\") >= start) & (F.col(\"_row_index\") < end)).drop(\"_row_index\")\n",
    "\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                batch_df.writeTo(f\"{catalog}.{table_name}\").append()\n",
    "                print(f\"âœ… Batch {start}-{end} appended successfully\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Attempt {attempt} for batch {start}-{end} failed: {e}\")\n",
    "                if attempt < max_retries:\n",
    "                    print(\"â³ Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    raise RuntimeError(f\"âŒ Failed to append batch {start}-{end} after {max_retries} attempts\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "756c302b-d388-45c6-8809-0d89b2542008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_votes(votes_dfs, table_name=\"votes\", catalog=\"nessie.silver\", repartitions=32, batch_size=50000):\n",
    "    if isinstance(votes_dfs, dict):\n",
    "        votes_dfs = list(votes_dfs.values())\n",
    "\n",
    "    combined = votes_dfs[0]\n",
    "    for df in votes_dfs[1:]:\n",
    "        combined = combined.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "    key_col = next((c for c in [\"id\", \"Id\"] if c in combined.columns), None)\n",
    "    window = Window.partitionBy(key_col).orderBy(F.desc(\"creation_date\"))\n",
    "    combined = combined.withColumn(\"rank\", F.row_number().over(window)).filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "    append_in_batches(combined, table_name, catalog=catalog, repartitions=repartitions, batch_size=batch_size)\n",
    "\n",
    "    final_df = spark.read.table(f\"{catalog}.{table_name}\")\n",
    "    final_df = final_df.withColumn(\"rank\", F.row_number().over(window)).filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "    final_df.writeTo(f\"{catalog}.{table_name}\").createOrReplace()\n",
    "    print(f\"âœ… Votes merged and deduplicated successfully in {catalog}.{table_name}\")\n",
    "\n",
    "\n",
    "def merge_posts(posts_dfs, table_name=\"posts\", catalog=\"nessie.silver\", repartitions=32, batch_size=50000):\n",
    "    if isinstance(posts_dfs, dict):\n",
    "        posts_dfs = list(posts_dfs.values())\n",
    "\n",
    "    combined = posts_dfs[0]\n",
    "    for df in posts_dfs[1:]:\n",
    "        combined = combined.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "    key_col = next((c for c in [\"id\", \"Id\"] if c in combined.columns), None)\n",
    "    window = Window.partitionBy(key_col).orderBy(F.desc(\"LastActivityDate\"))\n",
    "    combined = combined.withColumn(\"rank\", F.row_number().over(window)).filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "    append_in_batches(combined, table_name, catalog=catalog, repartitions=repartitions, batch_size=batch_size)\n",
    "\n",
    "    final_df = spark.read.table(f\"{catalog}.{table_name}\")\n",
    "    final_df = final_df.withColumn(\"rank\", F.row_number().over(window)).filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "    final_df.writeTo(f\"{catalog}.{table_name}\").createOrReplace()\n",
    "    print(f\"âœ… Posts merged and deduplicated successfully in {catalog}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eafb2107-8428-4ef1-aad9-bb1d9b96087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_dfs = {\n",
    "    \"posts_2022\": posts_2022_clean,\n",
    "    \"posts_2023\": posts_2023_clean,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49ddb580-6b92-47cf-800b-d1041d010a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_dfs = {\n",
    "    \"votes_2022\": votes_2022_clean,\n",
    "    \"votes_2023\": votes_2023_clean\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5cf42d0-e794-4862-9435-d8e8d7ed8d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DataFrame preparado con Ã­ndice para particionado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o482.count",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmerge_votes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvotes_dfs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m, in \u001b[0;36mmerge_votes\u001b[0;34m(votes_dfs, table_name, catalog, repartitions, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m window \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39mpartitionBy(key_col)\u001b[38;5;241m.\u001b[39morderBy(F\u001b[38;5;241m.\u001b[39mdesc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreation_date\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     11\u001b[0m combined \u001b[38;5;241m=\u001b[39m combined\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mrow_number()\u001b[38;5;241m.\u001b[39mover(window))\u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mappend_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatalog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatalog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepartitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m final_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mtable(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcatalog\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m final_df \u001b[38;5;241m=\u001b[39m final_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mrow_number()\u001b[38;5;241m.\u001b[39mover(window))\u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m, in \u001b[0;36mappend_in_batches\u001b[0;34m(df, table_name, catalog, repartitions, batch_size, max_retries)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Calcular nÃºmero de batches sin usar .count()\u001b[39;00m\n\u001b[1;32m     14\u001b[0m indexed_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_row_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m total_rows \u001b[38;5;241m=\u001b[39m \u001b[43mindexed_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mðŸ“Š Total rows to write in append: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_rows\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, total_rows, batch_size):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o482.count"
     ]
    }
   ],
   "source": [
    "merge_votes(votes_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e9d09-cf4e-4da5-b15a-65c4dc2c7157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688a560-0028-4218-963a-a83718f31555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
