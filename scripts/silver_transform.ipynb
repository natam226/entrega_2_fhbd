{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccbc1fed-a1ed-4414-a0d2-2b454ed903c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, explode, from_json, col, current_date,current_timestamp, lit, to_date, month\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, StructType, StructField, ArrayType\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82b7c792-b881-457a-ab40-640e1003ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_URI = \"http://nessie:19120/api/v1\"\n",
    "WAREHOUSE = \"s3a://silver/\"              \n",
    "STORAGE_URI = \"http://minio:9000\"\n",
    "AWS_ACCESS_KEY = \"admin\"\n",
    "AWS_SECRET_KEY = \"password\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9e79275-57d1-43d0-a8fc-6ca01db10936",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('silver_transform')\n",
    "\n",
    "        # üì¶ Dependencias necesarias\n",
    "        .set(\"spark.jars.packages\", \",\".join([\n",
    "            \"org.postgresql:postgresql:42.7.3\",\n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0\",\n",
    "            \"org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1\",\n",
    "            \"software.amazon.awssdk:bundle:2.24.8\",\n",
    "            \"software.amazon.awssdk:url-connection-client:2.24.8\",\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4\"\n",
    "        ]))\n",
    "\n",
    "        # üß© Extensiones Iceberg + Nessie\n",
    "        .set(\"spark.sql.extensions\", \",\".join([\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "            \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n",
    "        ]))\n",
    "\n",
    "        # üóÇÔ∏è Cat√°logo Nessie\n",
    "        .set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "        .set(\"spark.sql.catalog.nessie.uri\", CATALOG_URI)\n",
    "        .set(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "        .set(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "        .set(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\")\n",
    "        .set(\"spark.sql.catalog.nessie.warehouse\", WAREHOUSE)\n",
    "\n",
    "        # ‚òÅÔ∏è Configuraci√≥n S3A para MinIO\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", STORAGE_URI)\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "        .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "             \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "        .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "        # ‚ö° Optimizaciones de ejecuci√≥n\n",
    "        .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "        .set(\"spark.sql.parquet.filterPushdown\", \"true\")\n",
    "        .set(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"64\")  # üîß m√°s particiones para distribuir carga\n",
    "        .set(\"spark.sql.files.maxPartitionBytes\", \"64MB\")  # ‚öñÔ∏è reduce tama√±o de tarea para evitar saturaci√≥n\n",
    "\n",
    "        .set(\"spark.driver.memory\", \"5g\")                     # Driver usa hasta 5 GB (de los 6g disponibles)\n",
    "        .set(\"spark.executor.memory\", \"6g\")                   # Cada executor usa hasta 6 GB (de los 8g disponibles)\n",
    "        .set(\"spark.executor.cores\", \"4\")                     # M√°s n√∫cleos por executor para paralelismo\n",
    "        .set(\"spark.driver.maxResultSize\", \"2g\")              # Aumenta el l√≠mite de resultados del driver\n",
    "        .set(\"spark.network.timeout\", \"600s\")                 # Timeout m√°s generoso para cargas grandes\n",
    "        .set(\"spark.executor.heartbeatInterval\", \"60s\")       # Latido coherente con el timeout\n",
    "        \n",
    "        # ‚öôÔ∏è Escritura\n",
    "        .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f8ab72d-5b3c-4ba3-875d-b7523461f276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Started\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Session Started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a79a110-b3f5-4714-b5db-57eae55aed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Revisando s3a://bronze/posts/\n",
      "üì¶ 2 Parquet files found inposts:\n",
      "   üìÑ s3a://bronze/posts/2022/2022.parquet\n",
      "   üìÑ s3a://bronze/posts/2023/2023.parquet\n",
      "\n",
      "üîç Revisando s3a://bronze/votes/\n",
      "üì¶ 2 Parquet files found invotes:\n",
      "   üìÑ s3a://bronze/votes/_2022/1760303320.9018185.377ef04ff9.parquet\n",
      "   üìÑ s3a://bronze/votes/_2023/1760303382.8533797.761f32b28e.parquet\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from py4j.java_gateway import java_import\n",
    "\n",
    "    # Import Hadoop classes\n",
    "    java_import(spark._jvm, 'org.apache.hadoop.fs.FileSystem')\n",
    "    java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "    java_import(spark._jvm, 'java.net.URI')\n",
    "\n",
    "    # ‚úÖ Create S3A-aware FileSystem\n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark._jvm.java.net.URI(\"s3a://bronze/\"),\n",
    "        spark._jsc.hadoopConfiguration()\n",
    "    )\n",
    "\n",
    "    def list_parquet_files(base_path):\n",
    "        results = []\n",
    "        path = spark._jvm.org.apache.hadoop.fs.Path(base_path)\n",
    "\n",
    "        if not fs.exists(path):\n",
    "            print(f\"‚ö†Ô∏è Path does not exist: {base_path}\")\n",
    "            return results\n",
    "\n",
    "        def recurse(p):\n",
    "            try:\n",
    "                status_list = fs.listStatus(p)\n",
    "                for f in status_list:\n",
    "                    if f.isDirectory():\n",
    "                        recurse(f.getPath())\n",
    "                    elif f.getPath().getName().endswith(\".parquet\"):\n",
    "                        results.append(f.getPath().toString())\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error listing {p}: {e}\")\n",
    "\n",
    "        recurse(path)\n",
    "        return results\n",
    "\n",
    "    # ===============================\n",
    "    # List posts and votes folders\n",
    "    # ===============================\n",
    "    for subdir in [\"posts\", \"votes\"]:\n",
    "        base = f\"s3a://bronze/{subdir}/\"\n",
    "        print(f\"\\nüîç Revisando {base}\")\n",
    "\n",
    "        try:\n",
    "            path = spark._jvm.org.apache.hadoop.fs.Path(base)\n",
    "            status = fs.listStatus(path)\n",
    "\n",
    "            if len(status) == 0:\n",
    "                print(\"‚ö†Ô∏è The bucket exists but is empty.\")\n",
    "            else:\n",
    "                parquet_files = list_parquet_files(base)\n",
    "                if parquet_files:\n",
    "                    print(f\"üì¶ {len(parquet_files)} Parquet files found in{subdir}:\")\n",
    "                    for f in parquet_files:\n",
    "                        print(\"   üìÑ\", f)\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Not .parquet files found\", base)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Error listing bucket:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e1fd5f9-742c-47f7-8975-ee21dfe10049",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_paths = list_parquet_files(\"s3a://bronze/posts/\")\n",
    "votes_paths = list_parquet_files(\"s3a://bronze/votes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2b06d86-9b18-4d66-b1d9-de29686fe64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_year(paths, year):\n",
    "    pattern = re.compile(rf\"[_/\\.]{year}([_/\\.]|$)\")\n",
    "    return [p for p in paths if pattern.search(p)]\n",
    "\n",
    "votes_2022_paths = filter_by_year(votes_paths, 2022)\n",
    "votes_2023_paths = filter_by_year(votes_paths, 2023)\n",
    "posts_2022_paths = filter_by_year(posts_paths, 2022)\n",
    "posts_2023_paths = filter_by_year(posts_paths, 2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63235c72-4055-41e8-98e8-613fcd289401",
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_2022 = spark.read.parquet(*votes_2022_paths)\n",
    "votes_2023 = spark.read.parquet(*votes_2023_paths)\n",
    "posts_2022 = spark.read.parquet(*posts_2022_paths)\n",
    "posts_2023 = spark.read.parquet(*posts_2023_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a34ac7f-1789-4fdb-b79d-ab44acb82548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_votes_sql(df):\n",
    "    \"\"\"\n",
    "    Cleans and enriches the VOTES dataset using Spark SQL.\n",
    "    - Removes duplicates by 'id'\n",
    "    - Fills missing values (bounty_amount, user_id, vote_type_id, post_id)\n",
    "    - Normalizes timestamps and adds derived columns with distinct names\n",
    "    - Filters out invalid records\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üßπ Cleaning & enriching VOTES dataset with Spark SQL...\")\n",
    "\n",
    "        df.createOrReplaceTempView(\"raw_votes\")\n",
    "\n",
    "        # Compute median bounty_amount\n",
    "        median_bounty = 0\n",
    "        if df.filter(F.col(\"bounty_amount\").isNotNull()).count() > 0:\n",
    "            median_bounty = df.approxQuantile(\"bounty_amount\", [0.5], 0.1)[0]\n",
    "\n",
    "        cleaned_df = spark.sql(f\"\"\"\n",
    "            WITH base AS (\n",
    "                SELECT DISTINCT id, post_id, vote_type_id, creation_date, user_id, bounty_amount\n",
    "                FROM raw_votes\n",
    "                WHERE id IS NOT NULL AND creation_date IS NOT NULL\n",
    "            )\n",
    "            SELECT\n",
    "                id,\n",
    "                COALESCE(post_id, -1) AS post_id_clean,\n",
    "                COALESCE(vote_type_id, -1) AS vote_type_id_clean,\n",
    "                TO_TIMESTAMP(creation_date) AS creation_ts,\n",
    "                DATE_FORMAT(TO_TIMESTAMP(creation_date), 'yyyy-MM-dd') AS creation_date_str,\n",
    "                YEAR(TO_TIMESTAMP(creation_date)) AS creation_year,\n",
    "                CURRENT_TIMESTAMP() AS load_date,\n",
    "                COALESCE(user_id, -1) AS user_id_clean,\n",
    "                COALESCE(bounty_amount, {median_bounty}) AS bounty_amount_clean\n",
    "            FROM base\n",
    "        \"\"\")\n",
    "\n",
    "        print(\"‚úÖ VOTES cleaning complete with Spark SQL.\\n\")\n",
    "        return cleaned_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cleaning VOTES with Spark SQL: {e}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3dd5b5ee-dc7a-4d40-aa14-1099824b3c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning & enriching VOTES dataset with Spark SQL...\n",
      "‚úÖ VOTES cleaning complete with Spark SQL.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "votes_2022_clean = clean_votes_sql(votes_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35e89b94-4d9b-49e7-982c-d22492179cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning & enriching VOTES dataset with Spark SQL...\n",
      "‚úÖ VOTES cleaning complete with Spark SQL.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "votes_2023_clean = clean_votes_sql(votes_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5915703-8670-4e31-84bb-c6ddcda7a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_posts_sql(df):\n",
    "    \"\"\"\n",
    "    Cleans and enriches the POSTS dataset using Spark SQL for the Silver layer.\n",
    "    - Removes duplicates by Id\n",
    "    - Normalizes numeric types and fills nulls\n",
    "    - Converts date fields to timestamp\n",
    "    - Adds derived columns: creation_date_str, year, load_date\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üßπ Cleaning & enriching POSTS dataset with Spark SQL...\")\n",
    "\n",
    "        # Register temp view\n",
    "        df.createOrReplaceTempView(\"raw_posts\")\n",
    "\n",
    "        # Run SQL transformation\n",
    "        cleaned_df = spark.sql(\"\"\"\n",
    "            WITH base AS (\n",
    "                SELECT DISTINCT * FROM raw_posts\n",
    "            ),\n",
    "            casted AS (\n",
    "                SELECT\n",
    "                    Id,\n",
    "                    COALESCE(CAST(Score AS BIGINT), 0) AS Score,\n",
    "                    COALESCE(CAST(ViewCount AS BIGINT), 0) AS ViewCount,\n",
    "                    COALESCE(CAST(AnswerCount AS BIGINT), 0) AS AnswerCount,\n",
    "                    COALESCE(CAST(CommentCount AS BIGINT), 0) AS CommentCount,\n",
    "                    COALESCE(CAST(FavoriteCount AS BIGINT), 0) AS FavoriteCount,\n",
    "                    CASE WHEN Body IS NULL THEN '' ELSE CAST(Body AS STRING) END AS Body,\n",
    "                    TO_TIMESTAMP(CreationDate) AS CreationDate,\n",
    "                    TO_TIMESTAMP(LastEditDate) AS LastEditDate,\n",
    "                    TO_TIMESTAMP(LastActivityDate) AS LastActivityDate,\n",
    "                    TO_TIMESTAMP(CommunityOwnedDate) AS CommunityOwnedDate,\n",
    "                    TO_TIMESTAMP(ClosedDate) AS ClosedDate\n",
    "                FROM base\n",
    "            ),\n",
    "            enriched AS (\n",
    "                SELECT *,\n",
    "                       CreationDate AS creation_date,\n",
    "                       DATE_FORMAT(CreationDate, 'yyyy-MM-dd') AS creation_date_str,\n",
    "                       YEAR(CreationDate) AS year,\n",
    "                       CURRENT_TIMESTAMP() AS load_date\n",
    "                FROM casted\n",
    "                WHERE Id IS NOT NULL AND CreationDate IS NOT NULL\n",
    "            )\n",
    "            SELECT * FROM enriched\n",
    "        \"\"\")\n",
    "\n",
    "        print(\"‚úÖ POSTS cleaning complete with Spark SQL.\\n\")\n",
    "        return cleaned_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cleaning POSTS with Spark SQL: {e}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "484718e2-a5b9-4714-873c-69483f5a91da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning & enriching POSTS dataset with Spark SQL...\n",
      "‚úÖ POSTS cleaning complete with Spark SQL.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_2022_clean = clean_posts_sql(posts_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f26fc589-fc5a-4481-bfd0-dda8aeaf589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning & enriching POSTS dataset with Spark SQL...\n",
      "‚úÖ POSTS cleaning complete with Spark SQL.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_2023_clean = clean_posts_sql(posts_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fda08ace-cf77-444e-9e7a-76f9dbf03021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.silver\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6b6d1fc-c187-4275-9bc1-7f5321226135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Score: long (nullable = false)\n",
      " |-- ViewCount: long (nullable = false)\n",
      " |-- AnswerCount: long (nullable = false)\n",
      " |-- CommentCount: long (nullable = false)\n",
      " |-- FavoriteCount: long (nullable = false)\n",
      " |-- Body: string (nullable = true)\n",
      " |-- CreationDate: timestamp (nullable = true)\n",
      " |-- LastEditDate: timestamp (nullable = true)\n",
      " |-- LastActivityDate: timestamp (nullable = true)\n",
      " |-- CommunityOwnedDate: timestamp (nullable = true)\n",
      " |-- ClosedDate: timestamp (nullable = true)\n",
      " |-- creation_date: timestamp (nullable = true)\n",
      " |-- creation_date_str: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- load_date: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_2023_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de10141b-04c2-4468-92c6-3f40b0b5d7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- post_id_clean: long (nullable = false)\n",
      " |-- vote_type_id_clean: long (nullable = false)\n",
      " |-- creation_ts: timestamp (nullable = true)\n",
      " |-- creation_date_str: string (nullable = true)\n",
      " |-- creation_year: integer (nullable = true)\n",
      " |-- load_date: timestamp (nullable = false)\n",
      " |-- user_id_clean: long (nullable = false)\n",
      " |-- bounty_amount_clean: decimal(21,1) (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "votes_2023_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "afead887-aa7a-4675-aa82-bae050215b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar vistas temporales\n",
    "posts_2022_clean.createOrReplaceTempView(\"posts_2022\")\n",
    "posts_2023_clean.createOrReplaceTempView(\"posts_2023\")\n",
    "\n",
    "# Filtrar por los primeros 4 meses usando el nombre correcto de la columna\n",
    "filtered_posts_2022 = spark.sql(\"\"\"\n",
    "    SELECT * FROM posts_2022\n",
    "    WHERE MONTH(CreationDate) BETWEEN 1 AND 4\n",
    "\"\"\")\n",
    "\n",
    "filtered_posts_2023 = spark.sql(\"\"\"\n",
    "    SELECT * FROM posts_2023\n",
    "    WHERE MONTH(CreationDate) BETWEEN 1 AND 4\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c508bdd5-b961-4233-af99-ed061184d7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|month| count|\n",
      "+-----+------+\n",
      "|    1|275404|\n",
      "|    2|260478|\n",
      "|    3|279094|\n",
      "|    4|260778|\n",
      "+-----+------+\n",
      "\n",
      "+-----+------+\n",
      "|month| count|\n",
      "+-----+------+\n",
      "|    1|225487|\n",
      "|    2|198825|\n",
      "|    3|203410|\n",
      "|    4|173205|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar distribuci√≥n mensual en los DataFrames filtrados\n",
    "filtered_posts_2022.groupBy(month(\"CreationDate\").alias(\"month\")).count().orderBy(\"month\").show()\n",
    "filtered_posts_2023.groupBy(month(\"CreationDate\").alias(\"month\")).count().orderBy(\"month\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f688a560-0028-4218-963a-a83718f31555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_posts_sql(filtered_posts_2022, filtered_posts_2023, catalog=\"nessie.silver\", table_name=\"posts\"):\n",
    "    \"\"\"\n",
    "    Merges filtered POSTS datasets into a Nessie/Iceberg table using Spark SQL with deduplication and transactional upsert.\n",
    "\n",
    "    - Registers filtered views for each dataset (e.g., posts_2022 and posts_2023)\n",
    "    - Combines both datasets using UNION ALL\n",
    "    - Deduplicates by 'Id' keeping the most recent record based on 'LastActivityDate'\n",
    "    - If the target Iceberg table does not exist, creates it automatically\n",
    "    - Performs a MERGE INTO operation for transactional upsert\n",
    "\n",
    "    Args:\n",
    "        filtered_posts_2022 (pyspark.sql.DataFrame): Cleaned and filtered posts DataFrame for year 2022.\n",
    "        filtered_posts_2023 (pyspark.sql.DataFrame): Cleaned and filtered posts DataFrame for year 2023.\n",
    "        catalog (str, optional): Target Nessie catalog and layer (e.g., \"nessie.silver\"). Defaults to \"nessie.silver\".\n",
    "        table_name (str, optional): Target table name for merge. Defaults to \"posts\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        - Ensures a single record per 'Id' (latest by 'LastActivityDate').\n",
    "        - Supports incremental Silver-layer processing in a medallion data architecture.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Registrar vistas temporales\n",
    "        filtered_posts_2022.createOrReplaceTempView(\"posts_2022\")\n",
    "        filtered_posts_2023.createOrReplaceTempView(\"posts_2023\")\n",
    "\n",
    "        # Crear vista combinada deduplicada\n",
    "        spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TEMP VIEW posts_updates AS\n",
    "            WITH combined AS (\n",
    "                SELECT * FROM posts_2022\n",
    "                UNION ALL\n",
    "                SELECT * FROM posts_2023\n",
    "            ),\n",
    "            ranked AS (\n",
    "                SELECT *,\n",
    "                       ROW_NUMBER() OVER (\n",
    "                           PARTITION BY Id\n",
    "                           ORDER BY LastActivityDate DESC\n",
    "                       ) AS rank\n",
    "                FROM combined\n",
    "            )\n",
    "            SELECT * FROM ranked WHERE rank = 1\n",
    "        \"\"\")\n",
    "\n",
    "        # Verificar si la tabla ya existe\n",
    "        table_path = f\"{catalog}.{table_name}\"\n",
    "        table_exists = spark.catalog.tableExists(table_path)\n",
    "\n",
    "        if not table_exists:\n",
    "            print(f\"‚öôÔ∏è Table {table_path} not found. Creating it now...\")\n",
    "            spark.table(\"posts_updates\").writeTo(table_path).create()\n",
    "            print(f\"‚úÖ Table {table_path} created successfully.\")\n",
    "        else:\n",
    "            # Ejecutar MERGE INTO para actualizar/inserciones\n",
    "            spark.sql(f\"\"\"\n",
    "                MERGE INTO {table_path} AS target\n",
    "                USING posts_updates AS source\n",
    "                ON target.Id = source.Id\n",
    "                WHEN MATCHED THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED THEN INSERT *\n",
    "            \"\"\")\n",
    "            print(f\"‚úÖ Posts merged successfully into {table_path} using Iceberg MERGE\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error merging posts with MERGE INTO: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3b0205f-ae48-4b0b-9d34-e45654f315d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Posts merged successfully into nessie.silver.posts using Iceberg MERGE\n"
     ]
    }
   ],
   "source": [
    "merge_posts_sql(filtered_posts_2022, filtered_posts_2023, catalog=\"nessie.silver\", table_name=\"posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "39774ba1-b543-4d54-9fc3-e2d8c15365a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar vistas temporales\n",
    "votes_2022_clean.createOrReplaceTempView(\"votes_2022\")\n",
    "votes_2023_clean.createOrReplaceTempView(\"votes_2023\")\n",
    "\n",
    "# Ejecutar SQL para filtrar los primeros 4 meses\n",
    "filtered_votes_2022 = spark.sql(\"\"\"\n",
    "    SELECT * FROM votes_2022\n",
    "    WHERE MONTH(creation_ts) BETWEEN 1 AND 4\n",
    "\"\"\")\n",
    "\n",
    "filtered_votes_2023 = spark.sql(\"\"\"\n",
    "    SELECT * FROM votes_2023\n",
    "    WHERE MONTH(creation_ts) BETWEEN 1 AND 4\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4018124e-2809-4b32-9794-54e750eeb904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|month|  count|\n",
      "+-----+-------+\n",
      "|    1|1445524|\n",
      "|    2|1324142|\n",
      "|    3|1387773|\n",
      "|    4|1305101|\n",
      "+-----+-------+\n",
      "\n",
      "+-----+-------+\n",
      "|month|  count|\n",
      "+-----+-------+\n",
      "|    1|1256117|\n",
      "|    2|1129914|\n",
      "|    3|1173429|\n",
      "|    4| 942774|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificar distribuci√≥n mensual en los DataFrames filtrados\n",
    "filtered_votes_2022.groupBy(month(\"creation_ts\").alias(\"month\")).count().orderBy(\"month\").show()\n",
    "filtered_votes_2023.groupBy(month(\"creation_ts\").alias(\"month\")).count().orderBy(\"month\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c1e9d09-cf4e-4da5-b15a-65c4dc2c7157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_votes_sql(filtered_votes_2022, filtered_votes_2023, catalog=\"nessie.silver\", table_name=\"votes\"):\n",
    "    \"\"\"\n",
    "    Merges filtered VOTES datasets incrementally into an Iceberg/Nessie table using Spark SQL MERGE.\n",
    "\n",
    "    - Registers filtered views (votes_2022, votes_2023)\n",
    "    - Combines both datasets with UNION ALL\n",
    "    - Deduplicates by 'id', keeping the most recent record by 'creation_ts'\n",
    "    - Creates the table if it does not exist\n",
    "    - Performs an upsert (update/insert) merge into the target table\n",
    "\n",
    "    Args:\n",
    "        filtered_votes_2022 (pyspark.sql.DataFrame): Cleaned and filtered votes DataFrame for year 2022.\n",
    "        filtered_votes_2023 (pyspark.sql.DataFrame): Cleaned and filtered votes DataFrame for year 2023.\n",
    "        catalog (str, optional): Target Nessie catalog and layer (e.g., \"nessie.silver\"). Defaults to \"nessie.silver\".\n",
    "        table_name (str, optional): Target table name. Defaults to \"votes\".\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        - Ensures only one record per 'id' remains (latest by 'creation_ts').\n",
    "        - Automatically creates the table if it does not exist.\n",
    "        - Uses Iceberg MERGE for efficient incremental updates.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Registrar vistas filtradas\n",
    "        filtered_votes_2022.createOrReplaceTempView(\"votes_2022\")\n",
    "        filtered_votes_2023.createOrReplaceTempView(\"votes_2023\")\n",
    "\n",
    "        # Crear vista unificada con deduplicaci√≥n\n",
    "        merged_votes = spark.sql(\"\"\"\n",
    "            WITH combined AS (\n",
    "                SELECT * FROM votes_2022\n",
    "                UNION ALL\n",
    "                SELECT * FROM votes_2023\n",
    "            ),\n",
    "            ranked AS (\n",
    "                SELECT *,\n",
    "                       ROW_NUMBER() OVER (\n",
    "                           PARTITION BY id\n",
    "                           ORDER BY creation_ts DESC\n",
    "                       ) AS rank\n",
    "                FROM combined\n",
    "            )\n",
    "            SELECT * FROM ranked WHERE rank = 1\n",
    "        \"\"\")\n",
    "        merged_votes.createOrReplaceTempView(\"votes_updates\")\n",
    "\n",
    "        # Validar existencia de tabla destino\n",
    "        table_path = f\"{catalog}.{table_name}\"\n",
    "        table_exists = spark.catalog.tableExists(table_path)\n",
    "\n",
    "        if not table_exists:\n",
    "            print(f\"‚öôÔ∏è Table {table_path} not found. Creating it now...\")\n",
    "            merged_votes.writeTo(table_path).create()\n",
    "            print(f\"‚úÖ Table {table_path} created successfully.\")\n",
    "        else:\n",
    "            # Ejecutar MERGE INTO para actualizar/inserciones\n",
    "            spark.sql(f\"\"\"\n",
    "                MERGE INTO {table_path} AS target\n",
    "                USING votes_updates AS source\n",
    "                ON target.id = source.id\n",
    "                WHEN MATCHED THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED THEN INSERT *\n",
    "            \"\"\")\n",
    "            print(f\"‚úÖ Votes merged successfully into {table_path} using Iceberg MERGE\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error merging votes with MERGE INTO: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61ff57d0-ad18-40e0-9c46-921ec3f53c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Votes merged successfully into nessie.silver.votes using Iceberg MERGE\n"
     ]
    }
   ],
   "source": [
    "merge_votes_sql(filtered_votes_2022, filtered_votes_2023, catalog=\"nessie.silver\", table_name=\"votes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "26402c1e-e65e-4f56-a368-fb971e2d5334",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa64ce5-6614-4a6f-a3dd-f1e60c73b1ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
