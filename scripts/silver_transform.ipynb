{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccbc1fed-a1ed-4414-a0d2-2b454ed903c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, explode, from_json, col, current_date,current_timestamp, lit\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, StructType, StructField, ArrayType\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce5fe0e1-3e46-4dfe-af25-114f48896612",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame([(1, \"test\")], [\"id\", \"dummy\"]).writeTo(\"nessie.silver.test\").createOrReplace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82b7c792-b881-457a-ab40-640e1003ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_URI = \"http://nessie:19120/api/v1\"\n",
    "WAREHOUSE = \"s3a://silver/\"              \n",
    "STORAGE_URI = \"http://minio:9000\"\n",
    "AWS_ACCESS_KEY = \"admin\"\n",
    "AWS_SECRET_KEY = \"password\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9e79275-57d1-43d0-a8fc-6ca01db10936",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('silver_transform')\n",
    "\n",
    "        # üì¶ Dependencias necesarias para Spark + Iceberg + Nessie + AWS SDK + Hadoop AWS\n",
    "        .set(\"spark.jars.packages\", \",\".join([\n",
    "            \"org.postgresql:postgresql:42.7.3\",\n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0\",\n",
    "            \"org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1\",\n",
    "            \"software.amazon.awssdk:bundle:2.24.8\",\n",
    "            \"software.amazon.awssdk:url-connection-client:2.24.8\",\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4\"\n",
    "        ]))\n",
    "\n",
    "        # üß© Extensiones necesarias para Iceberg + Nessie\n",
    "        .set(\"spark.sql.extensions\", \",\".join([\n",
    "            \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "            \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions\"\n",
    "        ]))\n",
    "\n",
    "        # üóÇÔ∏è Configuraci√≥n del cat√°logo Nessie\n",
    "        .set(\"spark.sql.catalog.nessie\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .set(\"spark.sql.catalog.nessie.catalog-impl\", \"org.apache.iceberg.nessie.NessieCatalog\")\n",
    "        .set(\"spark.sql.catalog.nessie.uri\", CATALOG_URI)\n",
    "        .set(\"spark.sql.catalog.nessie.ref\", \"main\")\n",
    "        .set(\"spark.sql.catalog.nessie.authentication.type\", \"NONE\")\n",
    "        .set(\"spark.sql.catalog.nessie.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\")\n",
    "        .set(\"spark.sql.catalog.nessie.warehouse\", WAREHOUSE)\n",
    "\n",
    "        # ‚òÅÔ∏è Configuraci√≥n de Hadoop S3A para MinIO\n",
    "        .set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .set(\"spark.hadoop.fs.s3a.endpoint\", STORAGE_URI)\n",
    "        .set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .set(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY)\n",
    "        .set(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_KEY)\n",
    "        .set(\"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "             \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "        .set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "        # ‚ö° Optimizaciones de ejecuci√≥n y lectura\n",
    "        .set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")  # ‚úÖ habilita PyArrow\n",
    "        .set(\"spark.sql.parquet.filterPushdown\", \"true\")           # ‚úÖ lee solo columnas necesarias\n",
    "        .set(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
    "        .set(\"spark.sql.shuffle.partitions\", \"32\")                # üîß m√°s particiones para evitar sobrecarga\n",
    "        .set(\"spark.sql.files.maxPartitionBytes\", \"128MB\")        # ‚öñÔ∏è reduce tama√±o de tarea\n",
    "\n",
    "        # üîß Recursos\n",
    "        .set('spark.driver.memory', '8g')             # m√°s memoria para driver\n",
    "        .set('spark.executor.memory', '8g')           # m√°s memoria por executor\n",
    "        .set('spark.executor.cores', '2')             # cores por executor\n",
    "        .set('spark.driver.maxResultSize', '4g')      # evita overflow al traer datos grandes\n",
    "\n",
    "        # ‚öôÔ∏è Optimizaciones de escritura\n",
    "        .set('spark.sql.parquet.compression.codec', 'snappy')\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f8ab72d-5b3c-4ba3-875d-b7523461f276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Started\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Session Started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee82049-1b50-4b95-b3b0-131d67b3fff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://nessie:19120/api/v1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.catalog.nessie.uri\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a79a110-b3f5-4714-b5db-57eae55aed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Revisando s3a://bronze/posts/\n",
      "üì¶ 2 Parquet files found inposts:\n",
      "   üìÑ s3a://bronze/posts/2022/2022.parquet\n",
      "   üìÑ s3a://bronze/posts/2023/2023.parquet\n",
      "\n",
      "üîç Revisando s3a://bronze/votes/\n",
      "üì¶ 2 Parquet files found invotes:\n",
      "   üìÑ s3a://bronze/votes/_2022/1760206920.323453.f8f40d786f.parquet\n",
      "   üìÑ s3a://bronze/votes/_2023/1760206962.9017577.6d111cba5f.parquet\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from py4j.java_gateway import java_import\n",
    "\n",
    "    # Import Hadoop classes\n",
    "    java_import(spark._jvm, 'org.apache.hadoop.fs.FileSystem')\n",
    "    java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "    java_import(spark._jvm, 'java.net.URI')\n",
    "\n",
    "    # ‚úÖ Create S3A-aware FileSystem\n",
    "    fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark._jvm.java.net.URI(\"s3a://bronze/\"),\n",
    "        spark._jsc.hadoopConfiguration()\n",
    "    )\n",
    "\n",
    "    def list_parquet_files(base_path):\n",
    "        results = []\n",
    "        path = spark._jvm.org.apache.hadoop.fs.Path(base_path)\n",
    "\n",
    "        if not fs.exists(path):\n",
    "            print(f\"‚ö†Ô∏è Path does not exist: {base_path}\")\n",
    "            return results\n",
    "\n",
    "        def recurse(p):\n",
    "            try:\n",
    "                status_list = fs.listStatus(p)\n",
    "                for f in status_list:\n",
    "                    if f.isDirectory():\n",
    "                        recurse(f.getPath())\n",
    "                    elif f.getPath().getName().endswith(\".parquet\"):\n",
    "                        results.append(f.getPath().toString())\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error listing {p}: {e}\")\n",
    "\n",
    "        recurse(path)\n",
    "        return results\n",
    "\n",
    "    # ===============================\n",
    "    # List posts and votes folders\n",
    "    # ===============================\n",
    "    for subdir in [\"posts\", \"votes\"]:\n",
    "        base = f\"s3a://bronze/{subdir}/\"\n",
    "        print(f\"\\nüîç Revisando {base}\")\n",
    "\n",
    "        try:\n",
    "            path = spark._jvm.org.apache.hadoop.fs.Path(base)\n",
    "            status = fs.listStatus(path)\n",
    "\n",
    "            if len(status) == 0:\n",
    "                print(\"‚ö†Ô∏è The bucket exists but is empty.\")\n",
    "            else:\n",
    "                parquet_files = list_parquet_files(base)\n",
    "                if parquet_files:\n",
    "                    print(f\"üì¶ {len(parquet_files)} Parquet files found in{subdir}:\")\n",
    "                    for f in parquet_files:\n",
    "                        print(\"   üìÑ\", f)\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è Not .parquet files found\", base)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Error listing bucket:\", e)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e1fd5f9-742c-47f7-8975-ee21dfe10049",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_paths = list_parquet_files(\"s3a://bronze/posts/\")\n",
    "votes_paths = list_parquet_files(\"s3a://bronze/votes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2b06d86-9b18-4d66-b1d9-de29686fe64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_year(paths, year):\n",
    "    pattern = re.compile(rf\"[_/\\.]{year}([_/\\.]|$)\")\n",
    "    return [p for p in paths if pattern.search(p)]\n",
    "\n",
    "votes_2022_paths = filter_by_year(votes_paths, 2022)\n",
    "votes_2023_paths = filter_by_year(votes_paths, 2023)\n",
    "posts_2022_paths = filter_by_year(posts_paths, 2022)\n",
    "posts_2023_paths = filter_by_year(posts_paths, 2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63235c72-4055-41e8-98e8-613fcd289401",
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_2022 = spark.read.parquet(*votes_2022_paths)\n",
    "votes_2023 = spark.read.parquet(*votes_2023_paths)\n",
    "posts_2022 = spark.read.parquet(*posts_2022_paths)\n",
    "posts_2023 = spark.read.parquet(*posts_2023_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8f8de65-f287-4fda-9f18-6de6c3c2e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_posts(df):\n",
    "    \"\"\"\n",
    "    Limpieza y enriquecimiento temporal de POSTS (Silver Layer).\n",
    "    - Convierte CreationDate a timestamp y genera columnas auxiliares.\n",
    "    - A√±ade columnas: creation_date_str, year, load_date.\n",
    "    - Rellena valores nulos, normaliza tipos y elimina duplicados.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üßπ Cleaning & enriching POSTS dataset...\")\n",
    "\n",
    "        \n",
    "        # Eliminar duplicados\n",
    "        df = df.dropDuplicates([\"Id\"])\n",
    "\n",
    "        # Asegurar tipos num√©ricos coherentes\n",
    "        df = df.withColumn(\"Score\", F.coalesce(F.col(\"Score\").cast(\"long\"), F.lit(0))) \\\n",
    "               .withColumn(\"ViewCount\", F.coalesce(F.col(\"ViewCount\").cast(\"long\"), F.lit(0))) \\\n",
    "               .withColumn(\"AnswerCount\", F.coalesce(F.col(\"AnswerCount\").cast(\"long\"), F.lit(0))) \\\n",
    "               .withColumn(\"CommentCount\", F.coalesce(F.col(\"CommentCount\").cast(\"long\"), F.lit(0))) \\\n",
    "               .withColumn(\"FavoriteCount\", F.coalesce(F.col(\"FavoriteCount\").cast(\"long\"), F.lit(0)))\n",
    "\n",
    "        # Convertir 'Body' a string y reemplazar nulos por vac√≠o\n",
    "        df = df.withColumn(\"Body\", F.col(\"Body\").cast(\"string\"))\n",
    "        df = df.withColumn(\"Body\", F.when(F.col(\"Body\").isNull(), F.lit(\"\")).otherwise(F.col(\"Body\")))\n",
    "\n",
    "        # Convertir fechas\n",
    "        date_columns = [\n",
    "            \"CreationDate\", \"LastEditDate\", \"LastActivityDate\",\n",
    "            \"CommunityOwnedDate\", \"ClosedDate\"\n",
    "        ]\n",
    "        for col in date_columns:\n",
    "            if col in df.columns:\n",
    "                df = df.withColumn(col, F.to_timestamp(F.col(col)))\n",
    "\n",
    "        # Agregar columnas de fecha derivadas\n",
    "        df = df.withColumn(\"creation_date\", F.col(\"CreationDate\")) \\\n",
    "               .withColumn(\"creation_date_str\", F.date_format(F.col(\"creation_date\"), \"yyyy-MM-dd\")) \\\n",
    "               .withColumn(\"year\", F.year(F.col(\"creation_date\")))\n",
    "\n",
    "        print(\"‚úÖ POSTS cleaning & enrichment complete.\\n\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cleaning POSTS: {e}\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50c10d6e-4a40-4b63-a5ed-95391ad33872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_votes(df):\n",
    "    \"\"\"\n",
    "    Cleans and enriches the VOTES dataset for the Silver layer.\n",
    "    Includes timestamp normalization, temporal enrichment, and null handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üßπ Cleaning & enriching VOTES dataset...\")\n",
    "\n",
    "        # Remove duplicates based on 'id'\n",
    "        df = df.dropDuplicates([\"id\"])\n",
    "\n",
    "        # Handle missing values\n",
    "        # Replace null bounty_amount with median or 0\n",
    "        if df.filter(F.col(\"bounty_amount\").isNotNull()).count() > 0:\n",
    "            median_bounty = df.approxQuantile(\"bounty_amount\", [0.5], 0.1)[0]\n",
    "        else:\n",
    "            median_bounty = 0\n",
    "        df = df.fillna({\"bounty_amount\": median_bounty})\n",
    "\n",
    "        # Replace missing user_id or vote_type_id with -1 (unknown)\n",
    "        df = df.fillna({\"user_id\": -1, \"vote_type_id\": -1, \"post_id\": -1})\n",
    "\n",
    "        # Normalize timestamp and enrich with temporal columns\n",
    "        df = (\n",
    "            df.withColumn(\"creation_date\", F.to_timestamp(F.col(\"creation_date\")))\n",
    "              .withColumn(\"creation_date_str\", F.date_format(F.col(\"creation_date\"), \"yyyy-MM-dd\"))\n",
    "              .withColumn(\"year\", F.year(F.col(\"creation_date\")))\n",
    "              .withColumn(\"load_date\", F.current_timestamp())\n",
    "        )\n",
    "\n",
    "        # 4Filter invalid records\n",
    "        df = df.filter(F.col(\"id\").isNotNull() & F.col(\"creation_date\").isNotNull())\n",
    "\n",
    "        print(\"‚úÖ VOTES cleaning & enrichment complete.\\n\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cleaning VOTES: {e}\")\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dd5b5ee-dc7a-4d40-aa14-1099824b3c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning & enriching VOTES dataset...\n",
      "‚úÖ VOTES cleaning & enrichment complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "votes_2022_clean = clean_votes(votes_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35e89b94-4d9b-49e7-982c-d22492179cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning & enriching VOTES dataset...\n",
      "‚úÖ VOTES cleaning & enrichment complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "votes_2023_clean = clean_votes(votes_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "484718e2-a5b9-4714-873c-69483f5a91da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning & enriching POSTS dataset...\n",
      "‚úÖ POSTS cleaning & enrichment complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_2022_clean = clean_posts(posts_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f26fc589-fc5a-4481-bfd0-dda8aeaf589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning & enriching POSTS dataset...\n",
      "‚úÖ POSTS cleaning & enrichment complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "posts_2023_clean = clean_posts(posts_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fda08ace-cf77-444e-9e7a-76f9dbf03021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.silver\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cba6a6bc-8c88-4d24-a7a6-6cb32e2c28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_write_batches(df, table_name, catalog=\"nessie.silver\", repartitions=32, batch_size=100000, max_retries=3):\n",
    "    \"\"\"\n",
    "    Escribe un DataFrame en Nessie en batches para evitar saturar la conexi√≥n.\n",
    "    \"\"\"\n",
    "    df = df.repartition(repartitions, \"id\" if \"id\" in df.columns else \"Id\")\n",
    "    total_rows = df.count()\n",
    "    print(f\"Total rows to write: {total_rows}\")\n",
    "\n",
    "    start = 0\n",
    "    while start < total_rows:\n",
    "        end = start + batch_size\n",
    "        batch_df = df.limit(end).subtract(df.limit(start))  # toma el batch\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                batch_df.writeTo(f\"{catalog}.{table_name}\").createOrReplace()\n",
    "                print(f\"‚úÖ Batch {start}-{end} written successfully\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Attempt {attempt} for batch {start}-{end} failed: {e}\")\n",
    "                if attempt < max_retries:\n",
    "                    print(\"‚è≥ Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    raise RuntimeError(f\"Failed to write batch {start}-{end} after {max_retries} attempts\") from e\n",
    "        start = end\n",
    "\n",
    "def merge_votes(votes_dfs, table_name=\"votes\", catalog=\"nessie.silver\", repartitions=32, batch_size=100000):\n",
    "    \"\"\"\n",
    "    Merge de varios DataFrames de votes, eliminando duplicados y manteniendo √∫ltimo registro por Id.\n",
    "    \"\"\"\n",
    "    if isinstance(votes_dfs, dict):\n",
    "        votes_dfs = list(votes_dfs.values())\n",
    "\n",
    "    combined = votes_dfs[0]\n",
    "    for df in votes_dfs[1:]:\n",
    "        combined = combined.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "    key_col = \"id\" if \"id\" in combined.columns else \"Id\"\n",
    "    window = Window.partitionBy(key_col).orderBy(F.desc(\"creation_date\"))\n",
    "    combined = combined.withColumn(\"rank\", F.row_number().over(window))\\\n",
    "                       .filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "    safe_write_batches(combined, table_name, catalog=catalog, repartitions=repartitions, batch_size=batch_size)\n",
    "\n",
    "def merge_posts(posts_dfs, table_name=\"posts\", catalog=\"nessie.silver\", repartitions=32, batch_size=100000):\n",
    "    \"\"\"\n",
    "    Merge de varios DataFrames de posts, eliminando duplicados y manteniendo √∫ltimo registro por Id.\n",
    "    \"\"\"\n",
    "    if isinstance(posts_dfs, dict):\n",
    "        posts_dfs = list(posts_dfs.values())\n",
    "\n",
    "    combined = posts_dfs[0]\n",
    "    for df in posts_dfs[1:]:\n",
    "        combined = combined.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "    key_col = \"id\" if \"id\" in combined.columns else \"Id\"\n",
    "    window = Window.partitionBy(key_col).orderBy(F.desc(\"LastActivityDate\"))\n",
    "    combined = combined.withColumn(\"rank\", F.row_number().over(window))\\\n",
    "                       .filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "    safe_write_batches(combined, table_name, catalog=catalog, repartitions=repartitions, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "565e969c-7bf7-459d-af8b-e27ba027dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_posts(posts_dfs, table_name=\"posts\", catalog=\"nessie.silver\", repartitions=32):\n",
    "    \"\"\"\n",
    "    Merge de varios DataFrames de posts, eliminando duplicados y manteniendo √∫ltimo registro por Id.\n",
    "    - posts_dfs: lista o diccionario de DataFrames de posts (por a√±o o periodo)\n",
    "    - repartitions: n√∫mero de particiones para escribir en Nessie\n",
    "    \"\"\"\n",
    "    # Convertir diccionario a lista si es necesario\n",
    "    if isinstance(posts_dfs, dict):\n",
    "        posts_dfs = list(posts_dfs.values())\n",
    "    \n",
    "    # Combinar todos los DataFrames\n",
    "    combined = posts_dfs[0]\n",
    "    for df in posts_dfs[1:]:\n",
    "        combined = combined.unionByName(df, allowMissingColumns=True)\n",
    "    \n",
    "    # Mantener solo √∫ltimo registro por Id usando last_activity_date\n",
    "    window = Window.partitionBy(\"Id\").orderBy(F.desc(\"LastActivityDate\"))\n",
    "    combined = combined.withColumn(\"rank\", F.row_number().over(window))\\\n",
    "                       .filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "    \n",
    "    # Reparticiona para balancear la carga\n",
    "    combined = combined.repartition(repartitions, \"id\")\n",
    "    \n",
    "    # Escritura tipo MERGE en Nessie\n",
    "    combined.writeTo(f\"{catalog}.{table_name}\").createOrReplace()\n",
    "    print(f\"‚úÖ Posts merged and written to {catalog}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eafb2107-8428-4ef1-aad9-bb1d9b96087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_dfs = {\n",
    "    \"posts_2022\": posts_2022_clean,\n",
    "    \"posts_2023\": posts_2023_clean,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9db0ffb-3bd2-4c92-aaf3-de39f6161192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows to write: 5082178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Attempt 1 for batch 0-1000 failed: An error occurred while calling o475.createOrReplace\n",
      "‚è≥ Retrying in 5 seconds...\n",
      "‚ö†Ô∏è Attempt 2 for batch 0-1000 failed: [Errno 111] Connection refused\n",
      "‚è≥ Retrying in 5 seconds...\n",
      "‚ö†Ô∏è Attempt 3 for batch 0-1000 failed: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to write batch 0-1000 after 3 attempts",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m, in \u001b[0;36msafe_write_batches\u001b[0;34m(df, table_name, catalog, repartitions, batch_size, max_retries)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mbatch_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcreateOrReplace()\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m written successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:5719\u001b[0m, in \u001b[0;36mDataFrame.writeTo\u001b[0;34m(self, table)\u001b[0m\n\u001b[1;32m   5688\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5689\u001b[0m \u001b[38;5;124;03mCreate a write configuration builder for v2 sources.\u001b[39;00m\n\u001b[1;32m   5690\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5717\u001b[0m \u001b[38;5;124;03m... ).partitionedBy(\"col\").createOrReplace()\u001b[39;00m\n\u001b[1;32m   5718\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 5719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameWriterV2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/readwriter.py:2001\u001b[0m, in \u001b[0;36mDataFrameWriterV2.__init__\u001b[0;34m(self, df, table)\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msparkSession\n\u001b[0;32m-> 2001\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwriter \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m   called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m   :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmerge_posts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposts_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 61\u001b[0m, in \u001b[0;36mmerge_posts\u001b[0;34m(posts_dfs, table_name, catalog, repartitions, batch_size)\u001b[0m\n\u001b[1;32m     57\u001b[0m window \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39mpartitionBy(key_col)\u001b[38;5;241m.\u001b[39morderBy(F\u001b[38;5;241m.\u001b[39mdesc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLastActivityDate\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     58\u001b[0m combined \u001b[38;5;241m=\u001b[39m combined\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mrow_number()\u001b[38;5;241m.\u001b[39mover(window))\\\n\u001b[1;32m     59\u001b[0m                    \u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m \u001b[43msafe_write_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatalog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatalog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepartitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 24\u001b[0m, in \u001b[0;36msafe_write_batches\u001b[0;34m(df, table_name, catalog, repartitions, batch_size, max_retries)\u001b[0m\n\u001b[1;32m     22\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to write batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m attempts\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     25\u001b[0m start \u001b[38;5;241m=\u001b[39m end\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to write batch 0-1000 after 3 attempts"
     ]
    }
   ],
   "source": [
    "merge_posts(posts_dfs, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dee9c33-cc33-47a2-80db-8ec700be8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_votes(votes_dfs, table_name=\"votes\", catalog=\"nessie.silver\", repartitions=32):\n",
    "    \"\"\"\n",
    "    Merge de varios DataFrames de votes, eliminando duplicados y manteniendo √∫ltimo registro por Id.\n",
    "    - votes_dfs: lista o diccionario de DataFrames de votes (por a√±o o periodo)\n",
    "    - repartitions: n√∫mero de particiones para escribir en Nessie\n",
    "    \"\"\"\n",
    "    # Convertir diccionario a lista si es necesario\n",
    "    if isinstance(votes_dfs, dict):\n",
    "        votes_dfs = list(votes_dfs.values())\n",
    "    \n",
    "    # Combinar todos los DataFrames\n",
    "    combined = votes_dfs[0]\n",
    "    for df in votes_dfs[1:]:\n",
    "        combined = combined.unionByName(df, allowMissingColumns=True)\n",
    "    \n",
    "    # Mantener solo √∫ltimo registro por Id usando creation_date\n",
    "    if \"id\" in combined.columns:\n",
    "        window = Window.partitionBy(\"id\").orderBy(F.desc(\"creation_date\"))\n",
    "        combined = combined.withColumn(\"rank\", F.row_number().over(window))\\\n",
    "                           .filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "    elif \"Id\" in combined.columns:\n",
    "        window = Window.partitionBy(\"Id\").orderBy(F.desc(\"creation_date\"))\n",
    "        combined = combined.withColumn(\"rank\", F.row_number().over(window))\\\n",
    "                           .filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "    \n",
    "    # Reparticiona para balancear la carga\n",
    "    combined = combined.repartition(repartitions, \"id\" if \"id\" in combined.columns else \"Id\")\n",
    "    \n",
    "    # Escritura tipo MERGE en Nessie\n",
    "    combined.writeTo(f\"{catalog}.{table_name}\").createOrReplace()\n",
    "    print(f\"‚úÖ Votes merged and written to {catalog}.{table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49ddb580-6b92-47cf-800b-d1041d010a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "votes_dfs = {\n",
    "    \"votes_2022\": votes_2022_clean,\n",
    "    \"votes_2023\": votes_2023_clean\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1273194f-a72c-4b34-b197-da8bebb16916",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows to write: 27061227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 55090)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Attempt 1 for batch 0-100 failed: An error occurred while calling o495.createOrReplace\n",
      "‚è≥ Retrying in 5 seconds...\n",
      "‚ö†Ô∏è Attempt 2 for batch 0-100 failed: [Errno 111] Connection refused\n",
      "‚è≥ Retrying in 5 seconds...\n",
      "‚ö†Ô∏è Attempt 3 for batch 0-100 failed: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to write batch 0-100 after 3 attempts",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m, in \u001b[0;36msafe_write_batches\u001b[0;34m(df, table_name, catalog, repartitions, batch_size, max_retries)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mbatch_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcatalog\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcreateOrReplace()\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m written successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:5719\u001b[0m, in \u001b[0;36mDataFrame.writeTo\u001b[0;34m(self, table)\u001b[0m\n\u001b[1;32m   5688\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5689\u001b[0m \u001b[38;5;124;03mCreate a write configuration builder for v2 sources.\u001b[39;00m\n\u001b[1;32m   5690\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5717\u001b[0m \u001b[38;5;124;03m... ).partitionedBy(\"col\").createOrReplace()\u001b[39;00m\n\u001b[1;32m   5718\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 5719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameWriterV2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/readwriter.py:2001\u001b[0m, in \u001b[0;36mDataFrameWriterV2.__init__\u001b[0;34m(self, df, table)\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msparkSession\n\u001b[0;32m-> 2001\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwriter \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m   called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m   :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmerge_votes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvotes_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 43\u001b[0m, in \u001b[0;36mmerge_votes\u001b[0;34m(votes_dfs, table_name, catalog, repartitions, batch_size)\u001b[0m\n\u001b[1;32m     39\u001b[0m window \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39mpartitionBy(key_col)\u001b[38;5;241m.\u001b[39morderBy(F\u001b[38;5;241m.\u001b[39mdesc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreation_date\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     40\u001b[0m combined \u001b[38;5;241m=\u001b[39m combined\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mrow_number()\u001b[38;5;241m.\u001b[39mover(window))\\\n\u001b[1;32m     41\u001b[0m                    \u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m \u001b[43msafe_write_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatalog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatalog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepartitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepartitions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 24\u001b[0m, in \u001b[0;36msafe_write_batches\u001b[0;34m(df, table_name, catalog, repartitions, batch_size, max_retries)\u001b[0m\n\u001b[1;32m     22\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to write batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m attempts\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     25\u001b[0m start \u001b[38;5;241m=\u001b[39m end\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to write batch 0-100 after 3 attempts"
     ]
    }
   ],
   "source": [
    "merge_votes(votes_dfs, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64eb726-b07a-460f-bf9d-20411b65dae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9067e87-e8ae-4833-ac4b-80ed12e22723",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in votes_dfs.items():\n",
    "    print(df_name)\n",
    "    df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cf42d0-e794-4862-9435-d8e8d7ed8d54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
